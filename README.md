# DetailCaps-VLM
A code repository for a visual language model suitable for generating captions for small objects in transmission line images.
## Installation and Requirements
1.Clone this repository and navigate to the folder
```bash
git clone https://github.com/SunDesheng-SDS/DetailCaps-VLM.git
cd DetailCaps-VLM
```
2.Install necessary packages

Our code is modified based on the TinyLLaVA framework. Please refer to [TinyLLaVA](https://github.com/TinyLLaVA/TinyLLaVA_Factory) for this part.

## Get Started

1.Data Preparation

Our dataset mainly consists of two parts: pre-training and fine-tuning. The pre-training dataset [uses blip_laion_cc_sbu_558k](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/tree/main), and the fine-tuning dataset uses our self-built transmission line defect subtitle dataset. If you fine-tune your own model, please build it according to the format of the LLaVA dataset when building the fine-tuning dataset. If you need our transmission line dataset, please contact us privately.

2.Train

For training, you need to use the two files train_phi.sh and train_phi_lora.sh provided in the train folder. The train_phi.sh file is used for pre-training the model, and the train_phi_lora.sh file is used for fine-tuning the model. Just replace the dataset path with your dataset path and download the pre-trained [clip-vit-large-patch14-336](https://huggingface.co/openai/clip-vit-large-patch14-336/tree/main) and [phi-2](https://huggingface.co/microsoft/phi-2/tree/main) at the same time.

Note:The innovative framework mentioned in our paper is concentrated in the tinyllava\model\vision_tower\base.py file

3.Evaluation

During the evaluation phase, we used classic image captioning evaluation metrics including BLEU-n, CIDEr, ROUGE-L, and METEOR. Our evaluation code is in the evaluate file. We first need to use the inference code(inference.py) to generate captions for each image and save them as a JSON file. Then, we use the JSON file of the accurately labeled image captions and the inference JSON file generated by the model together with our evaluation code to calculate the metrics.

## Acknowledgement
We would like to thank Junlong Jia, Ying Hu, Xi Weng and others for building the [TinyLLaVA](https://github.com/TinyLLaVA/TinyLLaVA_Factory) framework.
## Community efforts
Our code repository is built on top of the [TinyLLaVA](https://github.com/TinyLLaVA/TinyLLaVA_Factory) project. Great work!

Our dataset format follows the [LLaVA](https://github.com/haotian-liu/LLaVA) instruction fine-tuning dataset format.Great work!



